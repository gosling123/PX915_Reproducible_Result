{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PX915 Individual Project Reproducible Result - Ben Gosling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "# import epoch calculators\n",
    "from multiprocessing import Pool\n",
    "import numpy as np\n",
    "import sdf\n",
    "from scipy import constants\n",
    "\n",
    "from Py_scripts.run_epoch import * # stores functions which aid in the running of epoch simulations\n",
    "from Py_scripts.sim_setup import * # stores functions which aid in the creation of setting up epoch runs\n",
    "\n",
    "# Gaussian process regression scripts\n",
    "from Py_scripts.gp import * # stores functions for performing GP regression for 1D input space\n",
    "from Py_scripts.utils import read_json_file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mu0 = constants.mu_0 # permeability of free space\n",
    "pi = np.pi # pi\n",
    "pico = 1e-12 # pico prefix (operating at pico-second time scale)\n",
    "micron = 1e-6 # micro prefix (operating at many microns for length scale)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EPOCH simulation set-up\n",
    "EPOCH requires you to specify an output directory which stores the input file to set up the simulation and store the output files. The python function below is used to create a directory within epoch_surra and populate it with one of the example input decks in the input_decks directory. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set name of the output directory\n",
    "dir = 'Data_epoch'\n",
    "sub_dirs = [f'Data_{i}' for i in range(1, 11)]\n",
    "dirs = [f'Data_epoch/Data_{i}' for i in range(1,11)]\n",
    "# input file/setup used throughout the report\n",
    "input_file = 'example_input.deck'\n",
    "# set initial laser intensity in W/cm^2 (varies between 1e14 - 1e16 in the report)\n",
    "intensity = 3e15 # set initial laser intensity in W/cm^2 \n",
    "# set density scale length in m (varies between 300e-6 - 100e-6 in the report) \n",
    "dens_scale_len = 500 * micron\n",
    "# set the number of particles per cell (set to 2048 in the report)\n",
    "# set to 100 to save time\n",
    "ppc = 100\n",
    "\n",
    "# For this example input deck, the number of timesteps and grid \n",
    "# cells are fixed at 4001 and 6473 respectively (see in example_input_deck)\n",
    "nx = 6473\n",
    "timesteps = 4001\n",
    "\n",
    "t_end = 2.0 * pico"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create sub-directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(sub_dirs)):\n",
    "    epoch_sim_sub_dir(dir = dir, sub_dir= sub_dirs[i], input_file = input_file, I = intensity, Ln = dens_scale_len, ppc = ppc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run all sub-directories at the same time using 2 processors each (20 in total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_epoch_in_parrallel():\n",
    "    pool = Pool(processes=len(dirs))\n",
    "    pool.map(run_epoch, dirs)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    run_epoch_in_parrallel()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extracting the Reflectivity - $\\mathcal{P}$\n",
    "\n",
    "The reflectivity ($\\mathcal{P}$) is defined as the ratio of the average back-scattered electromagnetic wave intensity to that of the intialised laser intensity:\n",
    "\n",
    "$$\n",
    "\\mathcal{P} = \\frac{\\langle I_{bs} \\rangle}{I_{L}} .\n",
    "$$\n",
    "\n",
    "The energy flux (i.e intensity) of a electromagnetic wave can be found from the Poynting vector ($\\mathbf{S}$), defined by the electric and magnetic fields of the electromagnetic wave:\n",
    "\n",
    "$$\n",
    "\\mathbf{S} = \\frac{1}{\\mu_0} \\mathbf{E} \\times \\mathbf{B}.\n",
    "$$\n",
    "\n",
    "The flux leaving in the $\\mathbf{x}$ direction is therefore given by:\n",
    "$$\n",
    "S_{x} = \\mathbf{S} \\cdot \\mathbf{\\hat{x}}  = \\frac{1}{\\mu_0} (E_{y}B_{z} -{E_{z}B_{y}}) \\approx \\frac{1}{\\mu_0} E_{y}B_{z},\n",
    "$$\n",
    "where the $E_z$ and $B_y$ fields being negliable as the backscattered light has the same polarisation as the laser (in this case the laser is polarised in the $y$-direction).\n",
    "\n",
    "### Read in Electric and Magnetic fields from all data files (timesteps) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_2D_Ey_field(dir):\n",
    "        # create space-time electric field array\n",
    "        Ey = np.zeros((nx, timesteps))\n",
    "        for i in range(timesteps):\n",
    "            fname = f'{dir}/fields_'+str(i).zfill(4)+'.sdf'\n",
    "            data = sdf.read(fname, dict = True)\n",
    "            Ey[:, i] = data['Electric Field/Ey'].data\n",
    "        return Ey\n",
    "        \n",
    "def get_2D_Bz_field(dir):\n",
    "        # create space-time electric field array\n",
    "        Bz = np.zeros((nx, timesteps))\n",
    "        for i in range(timesteps):\n",
    "            fname = f'{dir}/fields_'+str(i).zfill(4)+'.sdf'\n",
    "            data = sdf.read(fname, dict = True)\n",
    "            Bz[:, i] = data['Magnetic Field/Bz'].data\n",
    "        return Bz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Frequency Filtering \n",
    "\n",
    "Before we begin to implement the poynting flux formula, we need to be able to extract the electric and magnetic field contributions corresponding to the backscattered light. In 1D this is simplified as we only have two signals, one corresponding to the laser and the other to the backscattered light.\n",
    "\n",
    "The laser signal is well-defined (as we know the frequncey of the laser), so we aim to extract this signal from each field using a windowed sinc filter so that we can seperate the frequncies hidden within the outputted field signal.\n",
    "##### (Windowed sinc filter reference : http://www.dspguide.com/ch16/2.htm (Equation 16-4))\n",
    "\n",
    "A bandpass filter was constructed by convolving low-pass (lpf) and high-pass filter (hpf), whcih are defined in frequncey space as:\n",
    "\n",
    "$$\n",
    "A_{\\text{lpf}} = \\begin{cases}\n",
    "    1 ,& \\text{if } \\omega \\leq \\omega_c\\\\\n",
    "    0,              & \\omega > \\omega_c\n",
    "\\end{cases}\n",
    "\\,\\,\n",
    ";\n",
    "\\,\\,\n",
    "A_{\\text{hpf}} = \\begin{cases}\n",
    "    1 ,& \\text{if } \\omega \\geq \\omega_c\\\\\n",
    "    0,              & \\omega < \\omega_c\n",
    "\\end{cases}\n",
    "$$\n",
    "where $\\omega_{c}$ is some cut-off frequency. In real space, the low-pass and high-pass filters are mathematically defined as:\n",
    "\n",
    "1. #### Sinc-filter\n",
    "$$\n",
    " h_{i}\\left(\\omega\\right) = \\underbrace{\\frac{\\mathrm{sin}\\left(\\frac{2\\pi\\omega}{\\omega_{N}} \\left(i -M/2 \\right)\\right)}{\\left(i - M/2\\right)}}_\\text{Sinc filter} \\underbrace{\\left[0.42 - 0.5 \\text{cos}\\left(\\frac{2\\pi i}{M}\\right) + 0.08\\text{cos}\\left(\\frac{4\\pi i}{M}\\right)\\right]}_\\text{Blackman Window}\n",
    "$$\n",
    "\n",
    "2. #### low-pass filter\n",
    "$$\n",
    "h_{i}^{\\mathrm{lpf}} = \\frac{h_{i}\\left(\\omega = \\omega_{\\mathrm{ub}}\\right)}{\\sum_{j} h_{j}\\left(\\omega = \\omega_{\\mathrm{ub}}\\right)}, \\,\\, ; \\,\\, \\omega_{\\mathrm{ub}} = 1.15 \\omega_L\n",
    "$$\n",
    "\n",
    "2. #### high-pass filter\n",
    "$$\n",
    "h_{i}^{\\mathrm{hpf}} = - \\frac{h_{i}\\left(\\omega = \\omega_{\\mathrm{lb}}\\right)}{\\sum_{j} h_{j}\\left(\\omega = \\omega_{\\mathrm{lb}}\\right)}, \\,\\, ; \\,\\, \\omega_{\\mathrm{lb}} = 0.85 \\omega_L\n",
    "$$\n",
    "\n",
    "3. #### complete band-pass filter\n",
    "$$\n",
    "h^{\\text{bpf}} = h^{\\text{lpf}} * h^{\\text{hpf}}\n",
    "$$\n",
    "\n",
    "\n",
    "If the truncation is too abrupt, we may introduce ripples into the passband, which leads to desirable frequencies not being passed with their correct amplitudes, as well as ripples in the stop band, meaning that undesirable signals will be unevenly attenuated. This is why the filter kernel is convolved with a Black-man window to help smooth out the frequency response.\n",
    "\n",
    "Applying this band-pass filter to the $E_y (x,t)$ and $B_z (x,t)$ signals attenuates all frequencies outside of the range $0.85 \\omega_L \\leq \\omega \\leq 1.15 \\omega_L$, which represents the transmitted laser light.\n",
    "\n",
    "\n",
    "### Band-pass filter construction\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def winsincFIR(omega_c,omega_s,M):\n",
    "    # cutoff frequency shoudl be a fraction of sampling frequency\n",
    "    ker = np.sinc((omega_c / omega_s) * (np.arange(M) - (M - 1)/2))\n",
    "    # Blackman window used for smooting filter\n",
    "    ker *= np.blackman(M)\n",
    "    # unit gain at zero frequency (normalisation)\n",
    "    ker /= np.sum(ker) \n",
    "    return ker\n",
    "\n",
    "def bandpass(w0,bw,omega_s,M):\n",
    "    # Angular frequency used for NIF Laser\n",
    "    omega = 5.36652868179e+15\n",
    "    w0 = w0 * omega\n",
    "    bw = bw * omega\n",
    "    # upper and lower bound frequencies of bandpass\n",
    "    ub = w0 + (bw / 2)\n",
    "    lb = w0 - (bw / 2)\n",
    "    # create high-pass filter with cutoff at the lower-bound\n",
    "    # inverse low-pass filter\n",
    "    hhpf = -1 * winsincFIR(lb,omega_s,M) \n",
    "    hhpf[(M - 1) // 2] += 1\n",
    "    # create low-pass filter with cutoff at the upper-bound\n",
    "    hlpf = winsincFIR(ub,omega_s,M)\n",
    "    # convolve the two into a band-pass filter\n",
    "    h = np.convolve(hlpf, hhpf)\n",
    "    return h\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The laser signal is simply extracted by convolving the chosen field with the band-pass filter:\n",
    "\n",
    "\n",
    "$$\n",
    "E^{\\text{Laser}}_y(x,t) = E_y(x,t) * h^{\\text{bpf}}\n",
    "$$\n",
    "$$\n",
    "B^{\\text{Laser}}_z(x,t) = B_z(x,t) * h^{\\text{bpf}}\n",
    "$$\n",
    "\n",
    "with the backscatterd signals being extracted from simple subtraction of this laser signal from the original:\n",
    "\n",
    "$$\n",
    "E^{\\text{Back-scatter}}_y(x,t) = E_y(x,t) - E^{\\text{Laser}}_y(x,t)\n",
    "$$\n",
    "$$\n",
    "B^{\\text{Back-scatter}}_z(x,t) = B_z(x,t) - B^{\\text{Laser}}_z(x,t) \n",
    "$$\n",
    "\n",
    "### Extract back-sacttered signals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_filtered_signals(dir, laser = False):\n",
    "        # required fields\n",
    "        Ey = get_2D_Ey_field(dir) # Ey(x,t) field\n",
    "        Bz = get_2D_Bz_field(dir) # Bz(x,t) field\n",
    "\n",
    "        n,m = Ey.shape # array size\n",
    "        omega_0 = 1.0 # normalised laser frequency \n",
    "        omega_bw = 0.3 # bandswidth centred at laser frequency\n",
    "        T_end = t_end # sim end time\n",
    "        N = timesteps # number of time steps\n",
    "        dt = T_end/N # time step\n",
    "        omegaNyq = pi/dt # Nyquist Frequency\n",
    "        omega_s = 2*pi/dt # sampling frequency \n",
    "        M = 1001 # half length of the filter kernel (must be odd) \n",
    "\n",
    "        h = bandpass(omega_0,omega_bw,omegaNyq,M) #bandpass filter\n",
    "\n",
    "       \n",
    "        # Laser signals\n",
    "        Ey_laser = np.zeros((n, m))\n",
    "        Bz_laser = np.zeros((n, m))\n",
    "\n",
    "        # SRS signals\n",
    "        Ey_SRS = np.zeros((n, m))\n",
    "        Bz_SRS = np.zeros((n, m))\n",
    "\n",
    "        # Fill arrays with data\n",
    "        for i in range(n):\n",
    "            # laser signals\n",
    "            Ey_laser[i, :] = np.convolve(Ey[i,:],h,mode='same')\n",
    "            Bz_laser[i, :] = np.convolve(Bz[i,:],h,mode='same')\n",
    "            # SRS signals\n",
    "            Ey_SRS[i, :] = Ey[i,:] - Ey_laser[i,:]\n",
    "            Bz_SRS[i, :] = Bz[i,:] - Bz_laser[i,:]\n",
    "\n",
    "        if laser:    \n",
    "            return Ey_laser, Bz_laser\n",
    "        else:    \n",
    "            return Ey_SRS, Bz_SRS\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have the extracted signals, we can estimate the backsacttered energy flux. The electric and magnetic fields are recorded at many spatial locations over many time steps. So to estimate the total back-sactter intensity it becomes sensible to average over both time and space, such that:\n",
    "\n",
    "$$\n",
    "\\langle I_{bs} \\rangle = \\frac{\\sum_{i = 1}^{N_{x}} \\sum_{j = 1}^{N_{t}} E^{\\mathrm{Back-scatter}}_{y}\\left(x_i, t_j\\right)B^{\\mathrm{Back-scatter}}_{z}\\left(x_i, t_j\\right)}{N_x N_t \\mu_0}\n",
    "$$\n",
    "\n",
    "where $N_t$ and $N_x$ equate to the number of timesteps and grid cells in which the averaging is performed over.\n",
    "The time averaging is performed over all time, where as the spatial averaging is done over cells close to the left-hand (laser entry) boundary.\n",
    "\n",
    "\n",
    "Thus, $N_{x}$ and $N_{t}$ are taken to be 10 and 4001 respectively.\n",
    "\n",
    "(At low intensities, the recorded poynting flux is domianted by noise as we have little to no back-scattered light being generated, in which the poynting vector is found to fluctuate between postive and negative values over time. As we are only considering light that is scattered backwards, we only want to average over the negative poytning vectors in this case, which is done by simply setting any positive poynting vectors to zero when averaging.)\n",
    "\n",
    "### Extract reflectivity - $\\mathcal{P}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## N_x = 10\n",
    "## N_t = 4001\n",
    "def get_bsrs(dir, ncells = 10, refelctivity = True):\n",
    "    # get required field signals\n",
    "    Ey, Bz = get_filtered_signals(dir, laser = False)           \n",
    "    W_cm2 = 1e4 # Convert to W_cm2\n",
    "    factor = mu0*W_cm2 # Denominator of Sx\n",
    "    S = Ey*Bz/factor # poynting flux\n",
    "    # integrate/average over time at each grid point\n",
    "    sum_t = np.zeros(nx)\n",
    "    for i in range(timesteps):\n",
    "        sig = S[:,i]\n",
    "        indx = np.where(sig > 0) # only care for backward travelling flux\n",
    "        sig[indx] = 0\n",
    "        sum_t += sig\n",
    "    S_t_av = np.abs(sum_t)/timesteps\n",
    "    # for backward travelling signals, we want to average close to the left-hand boundary\n",
    "    sum_x = 0\n",
    "    for i in range(ncells):\n",
    "        sum_x += S_t_av[i]\n",
    "    S_av = sum_x/ncells\n",
    "    if refelctivity:\n",
    "        return S_av/intensity\n",
    "    else:\n",
    "        return S_av"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract $\\mathcal{P}$ for each EPOCH run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "P_data = np.array([])\n",
    "for dir in dirs:\n",
    "    P = get_bsrs(dir, ncells = 10, refelctivity=True)\n",
    "    print(f'result for dir = {dir}, P = {P}')\n",
    "    P_data = np.append(P_data, P)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convergence of Mean and Variance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def average(array, num):\n",
    "    data = array[:num]\n",
    "    n = len(data)\n",
    "    av = 0\n",
    "    for i in range(n):\n",
    "        av += data[i]\n",
    "    av /= n\n",
    "    return av\n",
    "\n",
    "def variance(array, num):\n",
    "    mean = average(array, num)\n",
    "    data = array[:num]\n",
    "    n = len(data)\n",
    "    sum_ = 0\n",
    "    for i in range(n):\n",
    "        sum_ += (data[i]-mean)**2\n",
    "    var = sum_/n\n",
    "    return var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "means = []\n",
    "variances = []\n",
    "N_samples = []\n",
    "for i in range(len(P_data)):\n",
    "    N_samples.append(i+1)\n",
    "    means.append(average(P_data, num = i+1))\n",
    "    variances.append(variance(P_data, num = i+1))\n",
    "    \n",
    "fig,ax = plt.subplots()\n",
    "ax.plot(N_samples, means, '-o', color ='b')\n",
    "ax.set_xlabel('Number of Sims')\n",
    "ax.set_ylabel('Mean Reflectivity')\n",
    "ax2.set_ylabel('Variance of Reflectivity')\n",
    "ax2=ax.twinx()\n",
    "ax2.plot(N_samples, variances,'-o', color = 'r')\n",
    "ax2.set_ylabel('Variance of Reflectivity')\n",
    "plt.gcf().set_size_inches(10,8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mean reflectivity and error from ten EPOCH samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "P_mean = np.mean(P_data)\n",
    "P_var = np.var(P_data)\n",
    "P_err = 2.0*np.sqrt(P_var)\n",
    "\n",
    "print(f'Mean reflectivity = {P_mean} W/cm^2')\n",
    "print(f'Varaiance of reflectivity = {P_var}')\n",
    "print(f'Error in reflectivity = {P_err} W/cm^2')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gaussian Process Regression Model\n",
    "\n",
    "Reflectivity Data was collected at many intensity and density scale length points, for which a Gaussian process model was trained on.\n",
    "\n",
    "We assume that the underlying function $f(x)$, can be represnted as a vector $\\mathbf{f}$, drawn from a multivariate normal distribution with zero mean and covariance matrix $\\mathbf{K}$:\n",
    "$$\n",
    "f \\sim \\mathcal{GP} \\rightarrow \\mathbf{f} \\sim \\mathcal{N}(\\mathbf{0}, \\mathbf{K}).\n",
    "$$\n",
    "As we can see from the EPOCH simulations, we have noisy observations due to sum inherent noise from plasma fluctuations. However, this noise is not homoscedastic (i.e not the same at each intensity), but varies depending on the set intensity. Thus, we define our noise observations in the following way:\n",
    "\n",
    "$$\n",
    " y_{i} = f\\left(\\bold{x}_{i}\\right) + \\epsilon_{i} \\,\\,;\\,\\,  \\epsilon_{i} | \\bold{x}_{i} \\sim \\mathcal{N}(0,\\,\\sigma_{n,i}^{2}),\n",
    "$$\n",
    "where $\\sigma^2_{n,i}$ is the measured noise variance at point $i$.\n",
    "\n",
    "Assuming additive independnece, the covariance matrix for the observations is augmented by this noise such that:\n",
    "$$\n",
    "\\Sigma [\\bold{y}] = K(X,X) + \\text{diag}(\\sigma^2_{n}(X)).\n",
    "$$\n",
    "\n",
    "### Noise GP Model \n",
    "To extract the estimated noise from plasma fluctuations at new locations, a further model was required for when we make predictions for the reflectivity at new points. For simplicity, this was done also using a Gaussian process, in which the noise model is trained on the recorded variances at each input space co-ordinate.\n",
    "\n",
    "The noise GP is defined similarly to the reflectivity GP model excpet for two asepcts. Firstly, the kernel functions used to describe $K$ for both models differ, the main GP uses a $\\textbf{Rational Quadratic}$ kernel, where as the noise model uses an $\\textbf{Exponential}$ kernel, as the noise model is expected to be less smooth in nature.  \n",
    "\n",
    "Secondly, the extracted variance observations are assumed to be exact as there is no way in which to sensibly quantify an error on the noise itself, so the covariance matrix for the noise observations is augmented by some small number, e.g:\n",
    "$$\n",
    "\\Sigma_{noise} [\\bold{y}] = K_{noise}(X_{noise},X_{noise}) + 1.0\\times 10^{-6} \\bold{I}.\n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set Training data for both GP models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This example is for a 1D test case of 20 logarithmically spaced intensity points in the range of $4.0\\times 10^{14} - 4.0\\times 10^{15} \\,\\, W/cm^2$. This was also done at the same resoloution as the EPOCH samples performed earlier in this notebook (PPC = 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input data (I, Ln)\n",
    "input_file = 'Training_data/train_inputs.json'\n",
    "# mean refelectivity at each input \n",
    "output_file = 'Training_data/train_outputs_mean.json'\n",
    "# recorded noise variance at each input\n",
    "var_file = 'Training_data/train_outputs_var.json'\n",
    "# fraction of data on which to train both GP's on\n",
    "train_frac = 0.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Call GP class contaning all functions for GP regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gp = LPI_GP(input_file=input_file, output_file=output_file,\\\n",
    "            var_file=var_file, train_frac=train_frac)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set/fix the training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gp.set_training_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyper-parameter optimisation\n",
    "\n",
    "Both the rational quadratic and exponential kernels have two hyper-parameter values which need to be determined (a lengthscale $l$, and a vaiance $\\sigma^{2}_{f}$).\n",
    "\n",
    "To set these values, a simple grid search routine is used for a space of lengthscales and variances, in which at each point on the grid, the ngeative log-likelihood function is estimated. For an assumed Gaussian likelihood distribution, the the ngeative log-likelihood function for  set of hyper-paramters $\\Theta = [l, \\sigma^2_{f}]$ is given by:\n",
    "\n",
    "$$\n",
    "\\mathcal{L}(\\Theta) = \\frac{1}{2}\\bold{y}^T\\left[\\bold{K} + \\bold{D}\\right]^{-1}\\bold{y} - \\frac{1}{2}\\mathrm{log}\\left|\\bold{K} + \\bold{D}\\right| - \\frac{N}{2}\\mathrm{log}{2\\pi},\n",
    "$$\n",
    "\n",
    "where $K = K$ and $D = \\text{diag}(\\sigma^2_n(X))$ for our general GP model and $K = K_{noise}$ and $D = 1.0\\times 10^{-6} \\bold{I}$, for the noise model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimise noise hyper-parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gp.optimise_noise_GP()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimise reflectivity model hyper-parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gp.optimise_GP()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### test train plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gp.test_train_plot()\n",
    "plt.gcf().set_size_inches(40,15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Prediction\n",
    "\n",
    "One can now make predictions at new locations $\\bold{X^*} = [\\bold{x^*}_1,.., \\bold{x^*}_M]$ giving new function values $\\bold{f^*} = [f(\\bold{x^*})_1,..,f(\\bold{x^*})_M]$ described via the Gaussian distribution:\n",
    "$$\n",
    "\\begin{bmatrix}\n",
    "\\bold{y} \\\\\n",
    "\\\\\n",
    "\\bold{f^*}  \n",
    "\\end{bmatrix}\n",
    "\\sim \\mathcal{N}\\left(\\bold{0}, \\begin{bmatrix}\n",
    "\\underbrace{\\bold{K}(\\bold{X}, \\bold{X})}_{\\bold{K}} + \\bold{D} & \\underbrace{\\bold{K}(\\bold{X}, \\bold{X^*})}_{\\bold{k_*}}\\\\\n",
    "\\\\\n",
    "\\underbrace{\\bold{K}(\\bold{X^*}, \\bold{X})}_{\\bold{k_*}^T} & \\underbrace{\\bold{K}(\\bold{X^*}, \\bold{X^*}}_{\\bold{K^*}})  \n",
    "\\end{bmatrix}\\right).\n",
    "$$\n",
    "Finally, applying this result to obtain the conditional distribution $\\bold{f^*}|\\bold{y}$, we find that the posterior distribution for $\\bold{f^*}$ is a Gaussian with mean and covariance given by:\n",
    "$$\n",
    "\\mathbb{E}[\\bold{f^*}] = \\bold{k_*}^T\\left[\\bold{K} + \\bold{D}\\right]^{-1}\\bold{y}\n",
    "$$\n",
    "$$\n",
    "\\Sigma[\\bold{f^*}] = \\bold{K^*} - \\bold{k_*}^T\\left[\\bold{K} + \\bold{D}\\right]^{-1} \\bold{k_*}.\n",
    "$$\n",
    "\n",
    "To account for the heteroscedastic noise nature of the\n",
    "problem, the covariance matrix must be augmented to\n",
    "include this noise as done earlier:\n",
    "$$\n",
    "\\Sigma[\\bold{f^*}] = \\bold{K^*} - \\bold{k_*}^T\\left[\\bold{K} + \\bold{D}\\right]^{-1} \\bold{k_*} + D^* \\,\\, \\text{where} \\,\\, D^* =  \\text{diag}(\\sigma^2_n(X^*)).\n",
    "$$\n",
    "\n",
    "Here, the estimation for the noise variance at the unkown loaction ($D^*$) is estimated from the noise GP model such that:\n",
    "$$\n",
    "\\sigma^{2}_n(X^*_i) = \\bold{k^{noise}_*}^T\\left[\\bold{K_{noise}} + \\bold{D_{noise}}\\right]^{-1}\\bold{y}\n",
    "$$\n",
    "which is equivalent to the mean of the posterior distribution from the noise GP model.\n",
    "\n",
    "----------------------------------------------------------------------------------------------------\n",
    "Note: To avoid having to invert any matricies, Cholesky decomposition was used such that the weights $\\alpha = \\left[\\bold{K} + \\bold{D}\\right]^{-1}y$, can be found in the following way:\n",
    "\n",
    "$$\n",
    "LL^{T} = \\left[\\bold{K} + \\bold{D}\\right] \\rightarrow \\left[\\bold{K} + \\bold{D}\\right]\\alpha = y \\rightarrow LL^T \\alpha = y\n",
    "$$\n",
    "$$\n",
    "\\alpha = L^T/(L/y)\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Show Prediction over whole training region"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_star = np.geomspace(4e14, 4e15, 100)[:,None]\n",
    "Y_star, V_epi, V_noise = gp.GP_predict(X_star, get_var=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inputs and mean ouputs that the reflectivity model is trained on \n",
    "X = np.exp(gp.get_input())\n",
    "Y = np.exp(gp.get_output())\n",
    "\n",
    "# all sample points (all 10 samples of each intensity point (20x10))\n",
    "X_all = np.exp(read_json_file('Training_data/all_inputs.json'))\n",
    "Y_all = np.exp(read_json_file('Training_data/all_outputs.json'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams[\"figure.figsize\"] = [14, 10]\n",
    "\n",
    "error_epi = 2.0*np.sqrt(V_epi)\n",
    "error_tot = 2.0*np.sqrt(V_epi + V_noise)\n",
    "\n",
    "Y_s = Y_star.flatten()\n",
    "X_s = X_star.flatten()\n",
    "\n",
    "plt.loglog(X_s, Y_s, color = 'blue', label = 'GP Mean')\n",
    "plt.fill_between(X_s, (Y_s-error_epi), (Y_s+error_epi), alpha = 0.3, color = 'cyan', label = 'Epistemic Error')\n",
    "plt.fill_between(X_s, (Y_s-error_tot), (Y_s+error_tot), alpha = 0.15, color = 'red', label = 'Total Error')\n",
    "plt.plot(X_all, Y_all, 'kx', color = 'red', label = 'All Samples', alpha = 0.8)\n",
    "plt.plot(X, Y, 'kx', color = 'blue', label = 'Mean Samples')\n",
    "plt.xlim(4e14, 4e15)\n",
    "plt.ylim(2e-3, 2e-1)\n",
    "\n",
    "plt.ylabel(r'Reflectivity - $\\mathcal{P}$')\n",
    "plt.xlabel(r'$I_{L} \\,\\, W/cm^{2}$')\n",
    "plt.legend(loc = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Error Bar validation\n",
    "\n",
    "Here we comapre the extracted error bars from the GP model with the recorded error from our EPOCH sampled at the same intensity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_star, V_epi, V_noise = gp.GP_predict(X_star = np.array([intensity])[:,None], get_var=True)\n",
    "error_epi = 2.0*np.sqrt(V_epi)\n",
    "error_noise = 2.0*np.sqrt(V_noise)\n",
    "error_tot = 2.0*np.sqrt(V_epi + V_noise)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "err_compare = [P_err, error_epi, error_noise, error_tot]\n",
    "# plot scaled sensitivities\n",
    "fig, ax = plt.subplots(figsize=(10,6))\n",
    "ax.plot(['Samaple Error', 'GP Epistemic Error', 'GP Noise Error', 'GP Total Error'], err_compare, 'o')\n",
    "ax.set_xticklabels(['Samaple Error', 'GP Epistemic Error', 'GP Noise Error', 'GP Total Error'], rotation=90)\n",
    "ax.set_ylabel(r'Error')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e7370f93d1d0cde622a1f8e1c04877d8463912d04d973331ad4851f04de6915a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
